{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVARL19_part4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsaXz-ENaLbx",
        "colab_type": "text"
      },
      "source": [
        "# Exploration in Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHhiMrobaW8d",
        "colab_type": "code",
        "outputId": "ed6035b7-2283-407a-de68-bfb97b685476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!rm -rf mvarl_hands_on/\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git\n",
        "!cd mvarl_hands_on/ && git fetch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mvarl_hands_on'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/122)\u001b[K\rremote: Counting objects:   1% (2/122)\u001b[K\rremote: Counting objects:   2% (3/122)\u001b[K\rremote: Counting objects:   3% (4/122)\u001b[K\rremote: Counting objects:   4% (5/122)\u001b[K\rremote: Counting objects:   5% (7/122)\u001b[K\rremote: Counting objects:   6% (8/122)\u001b[K\rremote: Counting objects:   7% (9/122)\u001b[K\rremote: Counting objects:   8% (10/122)\u001b[K\rremote: Counting objects:   9% (11/122)\u001b[K\rremote: Counting objects:  10% (13/122)\u001b[K\rremote: Counting objects:  11% (14/122)\u001b[K\rremote: Counting objects:  12% (15/122)\u001b[K\rremote: Counting objects:  13% (16/122)\u001b[K\rremote: Counting objects:  14% (18/122)\u001b[K\rremote: Counting objects:  15% (19/122)\u001b[K\rremote: Counting objects:  16% (20/122)\u001b[K\rremote: Counting objects:  17% (21/122)\u001b[K\rremote: Counting objects:  18% (22/122)\u001b[K\rremote: Counting objects:  19% (24/122)\u001b[K\rremote: Counting objects:  20% (25/122)\u001b[K\rremote: Counting objects:  21% (26/122)\u001b[K\rremote: Counting objects:  22% (27/122)\u001b[K\rremote: Counting objects:  23% (29/122)\u001b[K\rremote: Counting objects:  24% (30/122)\u001b[K\rremote: Counting objects:  25% (31/122)\u001b[K\rremote: Counting objects:  26% (32/122)\u001b[K\rremote: Counting objects:  27% (33/122)\u001b[K\rremote: Counting objects:  28% (35/122)\u001b[K\rremote: Counting objects:  29% (36/122)\u001b[K\rremote: Counting objects:  30% (37/122)\u001b[K\rremote: Counting objects:  31% (38/122)\u001b[K\rremote: Counting objects:  32% (40/122)\u001b[K\rremote: Counting objects:  33% (41/122)\u001b[K\rremote: Counting objects:  34% (42/122)\u001b[K\rremote: Counting objects:  35% (43/122)\u001b[K\rremote: Counting objects:  36% (44/122)\u001b[K\rremote: Counting objects:  37% (46/122)\u001b[K\rremote: Counting objects:  38% (47/122)\u001b[K\rremote: Counting objects:  39% (48/122)\u001b[K\rremote: Counting objects:  40% (49/122)\u001b[K\rremote: Counting objects:  41% (51/122)\u001b[K\rremote: Counting objects:  42% (52/122)\u001b[K\rremote: Counting objects:  43% (53/122)\u001b[K\rremote: Counting objects:  44% (54/122)\u001b[K\rremote: Counting objects:  45% (55/122)\u001b[K\rremote: Counting objects:  46% (57/122)\u001b[K\rremote: Counting objects:  47% (58/122)\u001b[K\rremote: Counting objects:  48% (59/122)\u001b[K\rremote: Counting objects:  49% (60/122)\u001b[K\rremote: Counting objects:  50% (61/122)\u001b[K\rremote: Counting objects:  51% (63/122)\u001b[K\rremote: Counting objects:  52% (64/122)\u001b[K\rremote: Counting objects:  53% (65/122)\u001b[K\rremote: Counting objects:  54% (66/122)\u001b[K\rremote: Counting objects:  55% (68/122)\u001b[K\rremote: Counting objects:  56% (69/122)\u001b[K\rremote: Counting objects:  57% (70/122)\u001b[K\rremote: Counting objects:  58% (71/122)\u001b[K\rremote: Counting objects:  59% (72/122)\u001b[K\rremote: Counting objects:  60% (74/122)\u001b[K\rremote: Counting objects:  61% (75/122)\u001b[K\rremote: Counting objects:  62% (76/122)\u001b[K\rremote: Counting objects:  63% (77/122)\u001b[K\rremote: Counting objects:  64% (79/122)\u001b[K\rremote: Counting objects:  65% (80/122)\u001b[K\rremote: Counting objects:  66% (81/122)\u001b[K\rremote: Counting objects:  67% (82/122)\u001b[K\rremote: Counting objects:  68% (83/122)\u001b[K\rremote: Counting objects:  69% (85/122)\u001b[K\rremote: Counting objects:  70% (86/122)\u001b[K\rremote: Counting objects:  71% (87/122)\u001b[K\rremote: Counting objects:  72% (88/122)\u001b[K\rremote: Counting objects:  73% (90/122)\u001b[K\rremote: Counting objects:  74% (91/122)\u001b[K\rremote: Counting objects:  75% (92/122)\u001b[K\rremote: Counting objects:  76% (93/122)\u001b[K\rremote: Counting objects:  77% (94/122)\u001b[K\rremote: Counting objects:  78% (96/122)\u001b[K\rremote: Counting objects:  79% (97/122)\u001b[K\rremote: Counting objects:  80% (98/122)\u001b[K\rremote: Counting objects:  81% (99/122)\u001b[K\rremote: Counting objects:  82% (101/122)\u001b[K\rremote: Counting objects:  83% (102/122)\u001b[K\rremote: Counting objects:  84% (103/122)\u001b[K\rremote: Counting objects:  85% (104/122)\u001b[K\rremote: Counting objects:  86% (105/122)\u001b[K\rremote: Counting objects:  87% (107/122)\u001b[K\rremote: Counting objects:  88% (108/122)\u001b[K\rremote: Counting objects:  89% (109/122)\u001b[K\rremote: Counting objects:  90% (110/122)\u001b[K\rremote: Counting objects:  91% (112/122)\u001b[K\rremote: Counting objects:  92% (113/122)\u001b[K\rremote: Counting objects:  93% (114/122)\u001b[K\rremote: Counting objects:  94% (115/122)\u001b[K\rremote: Counting objects:  95% (116/122)\u001b[K\rremote: Counting objects:  96% (118/122)\u001b[K\rremote: Counting objects:  97% (119/122)\u001b[K\rremote: Counting objects:  98% (120/122)\u001b[K\rremote: Counting objects:  99% (121/122)\u001b[K\rremote: Counting objects: 100% (122/122)\u001b[K\rremote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 122 (delta 49), reused 109 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (122/122), 186.49 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jpzHHRYd0pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import os\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from gridworld import GridWorldWithPits\n",
        "from tqdm import tqdm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNxcIZtaSZJ",
        "colab_type": "text"
      },
      "source": [
        "## Finite-Horizon Tabular MDPs\n",
        "We consider finite horizon problems with horizon $H$.For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Suggestion:\n",
        "- $V$ -> $(H+1, S)$-dimensional matrix\n",
        "- deterministic policy $\\pi$ -> $(H, S)$-dimensional matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smh0opCibDY1",
        "colab_type": "text"
      },
      "source": [
        "**Question 1:** implement backward induction for $V^\\pi$ and $V^\\star$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhLaiao3aR4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A,S)-dim matrix\n",
        "            H: horizon\n",
        "            policy: a deterministic policy (H, S)-dim matrix\n",
        "            \n",
        "        Returns:\n",
        "            The V-function of the provided policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    states=np.arange(S)\n",
        "\n",
        "    # Initialize V\n",
        "    V = np.zeros((H + 1, S))\n",
        "\n",
        "    # Evaluate the policy (going backward since V(h) is based on V(h+1))\n",
        "    for h in reversed(range(H)):\n",
        "        actions = policy[h].astype('int')\n",
        "        V[h] = np.sum(P[states, actions] * (R[states, actions] + V[h + 1]), axis=1)\n",
        "    return V\n",
        "\n",
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A,S)-dim matrix\n",
        "            H: horizon\n",
        "            \n",
        "        Returns:\n",
        "            The optimal V-function\n",
        "            The optimal policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    states=np.arange(S)\n",
        "\n",
        "    # Initialize optimal V and policy\n",
        "    opt_V = np.zeros((H + 1, S))\n",
        "    opt_policy = np.zeros((H, S)).astype('int')\n",
        "\n",
        "    # Evaluate Q and deduce optimal V and policy\n",
        "    for h in reversed(range(H)):\n",
        "        Q = np.sum(P*(R+opt_V[h+1]), axis=2)\n",
        "\n",
        "        opt_policy[h] = np.argmax(Q,axis=1)\n",
        "        opt_V[h] = Q[states,opt_policy[h]]\n",
        "\n",
        "    return opt_V, opt_policy\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_l6lqhDbYmQ",
        "colab_type": "text"
      },
      "source": [
        "Let's set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkOs-0y_bd61",
        "colab_type": "code",
        "outputId": "ffcd416f-0ec3-422a-ac91-4ba498208307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\n",
        "H = 6\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paggla09bl6S",
        "colab_type": "text"
      },
      "source": [
        "We should test previous functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgeanS2NbpQg",
        "colab_type": "code",
        "outputId": "70bb7002-e12d-4c22-b9aa-617ad79218ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "V, optimal_pol = backward_induction(env.P, env.R, H)\n",
        "print(V)\n",
        "Vpi = evaluate_policy(env.P, env.R, H, optimal_pol)\n",
        "print(Vpi)\n",
        "assert np.allclose(V, Vpi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3541c447ee66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_pol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_induction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mVpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_pol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-843bc96b3f8b>\u001b[0m in \u001b[0;36mbackward_induction\u001b[0;34m(P, R, H)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Evaluate Q and deduce optimal V and policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mopt_V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mopt_policy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (12,4) (12,) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACThGeMb48-",
        "colab_type": "text"
      },
      "source": [
        "Run the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbyoa9Qpb6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(H):\n",
        "    next_state, reward, _, _ = env.step(optimal_pol[i, state])\n",
        "    env.render()\n",
        "    state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZtdzpwUcHYj",
        "colab_type": "text"
      },
      "source": [
        "Finally we are ready to implement our exploration algorithm.\n",
        "\n",
        "**Question 2**: implement the **UCB-VI** algorithm.\n",
        "\n",
        "UCBVI is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "Where $N$ is the number of times that we observe state $s_{k,h}$ then $s_{k,h+1}$ when taking action $a_{k,h}$.\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "\n",
        "UCBVI exploits exploration bonus to perform optimistic planning on the empirical MDP $(\\hat{p}, \\hat{r})$.\n",
        "The optimal Q-function of this MDP can be obtained using backward induction.\n",
        "\n",
        "The optimal Bellman operator for Q-function is defined similarly as\n",
        "$$\n",
        "Q_h^\\star(s,a) =  b(s,a) + \\sum_{s'} p(s'|s,a) \\left( r(s,a,s') + \\max_{a'} Q_{h+1}^\\star(s',a')\\right) \n",
        "$$\n",
        "where $Q_{H+1}(s,a) = 0$ and $b$ is an exploration bonus.\n",
        "\n",
        "<font color='#ed7d31'>Exploration Bonus</font>\n",
        "\n",
        "Using Hoeffding's bound we have that\n",
        "$$\n",
        "b_{k,h}(s,a) = 7(H-h+1)L\\sqrt{\\frac{1}{N_k(s,a)}}\n",
        "$$\n",
        "where $L = \\ln(5SAT/\\delta)$.\n",
        "\n",
        "A tighter exploration bonus is obtained using Bernstein's bound. Since it's expression is much more complicated, we provided the code (see `compute_bernstein_bonus`).\n",
        "\n",
        "\n",
        "Refer to [Minimax Regret Bounds for Reinforcement Learning](https://arxiv.org/abs/1703.05449) for additional details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUi83GDkwd7k",
        "colab_type": "text"
      },
      "source": [
        "**For T** we chose $K \\times H$ like in the above paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kwvl2YkUvqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCBVI:\n",
        "    def __init__(self, config):\n",
        "        np.random.seed(seed=config[\"seed\"])\n",
        "        self.env = config[\"env\"]\n",
        "        self.horizon = config[\"horizon\"]\n",
        "        self.scale_factor = config[\"scale_factor\"]\n",
        "        self.nb_repetitions = config[\"repetitions\"]\n",
        "        self.nb_episodes = config[\"episodes\"]\n",
        "        assert config[\"b_type\"] in [\"hoeffding\", \"bernstein\"]\n",
        "        self.b_type = config[\"b_type\"]\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        S, A = self.env.Ns, self.env.Na\n",
        "        self.delta = 0.1\n",
        "        self.t = 0\n",
        "        self.episode = 0\n",
        "        self.Phat = np.zeros((S, A, S))\n",
        "        self.Rhat = np.zeros((S, A, S))\n",
        "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
        "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
        "        self.policy = np.zeros((self.horizon, S), dtype=np.int)\n",
        "        self.V = np.zeros((self.horizon + 1, S))\n",
        "        self.Q = np.zeros((self.horizon + 1, S, A))\n",
        "        self.bonus = np.zeros((self.horizon, S, A))\n",
        "\n",
        "    def get_optimistic_q(self):\n",
        "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
        "        \"\"\"\n",
        "        H = self.horizon\n",
        "        S, A = self.N_sa.shape\n",
        "        self.V.fill(0)\n",
        "        self.Q.fill(0)\n",
        "\n",
        "        if self.b_type == \"hoeffding\":\n",
        "            self.compute_hoeffding_bonus()\n",
        "\n",
        "        for h in reversed(range(H)):\n",
        "            if self.b_type == \"bernstein\":\n",
        "                self.compute_bernstein_bonus(h, self.V[h + 1])\n",
        "\n",
        "            self.Q[h] = self.bonus[h] + np.sum(\n",
        "                self.Phat * (self.Rhat + self.Q[h + 1].max(1)), axis=-1\n",
        "            )\n",
        "            self.policy[h] = self.Q[h].argmax(1)\n",
        "            self.V[h] = self.Q[h, np.arange(S), self.policy[h]]\n",
        "            self.V[h] = np.minimum(H - h + 2.0, self.V[h])\n",
        "\n",
        "    def compute_hoeffding_bonus(self):\n",
        "        \"\"\"Compute Hoeffding-based exploration bonus\n",
        "        \"\"\"\n",
        "        S, A = self.env.Ns, self.env.Na\n",
        "        N = np.maximum(1.0, self.N_sa)\n",
        "        # L = np.log(5 * S * A * max(self.t, 1) / self.delta)\n",
        "        L = np.log(5.0 * S * A * N / self.delta)\n",
        "\n",
        "        for h in range(self.horizon):\n",
        "            self.bonus[h] = (\n",
        "                self.scale_factor * 7.0 * (self.horizon - h + 2.0) * np.sqrt(L / N)\n",
        "            )\n",
        "\n",
        "    def compute_bernstein_bonus(self, h, Vhp1):\n",
        "        \"\"\"Compute Bernstein-based exploration bonus\n",
        "\n",
        "        Parameters:\n",
        "            h: state\n",
        "            Vhp1: value function at state h+1 (S-dim vector)\n",
        "        \"\"\"\n",
        "        S, A = self.N_sa.shape\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
        "                n = max(1, self.N_sa[s][a])\n",
        "                var, mean = 0, 0\n",
        "                for i in range(S):\n",
        "                    mean += self.Phat[s, a, i] * Vhp1[i]\n",
        "                for i in range(S):\n",
        "                    var += self.Phat[s, a, i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\n",
        "                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon - h + 2) / (\n",
        "                    3 * n\n",
        "                )\n",
        "                T2 = np.sqrt(8 * (self.horizon - h + 2) ** 2 / n)\n",
        "                self.bonus[h, s, a] = self.scale_factor * (T1 + T2)\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the internal statistics\n",
        "        \"\"\"\n",
        "        self.N_sas[state, action, next_state] += 1\n",
        "        self.N_sa[state, action] += 1\n",
        "        self.Phat[state, action, next_state] = (\n",
        "            self.N_sas[state, action, next_state] / self.N_sa[state, action]\n",
        "        )\n",
        "        self.Rhat[state, action, next_state] = (\n",
        "            (self.N_sas[state, action, next_state] - 1)\n",
        "            * self.Rhat[state, action, next_state]\n",
        "            + reward\n",
        "        ) / self.N_sas[state, action, next_state]\n",
        "\n",
        "    def run_episode(self):\n",
        "        episode_reward = 0\n",
        "        state = self.env.reset()\n",
        "        initial_state = state\n",
        "        self.get_optimistic_q()\n",
        "\n",
        "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
        "        self.episode_value.append(Vpi[0, initial_state])\n",
        "\n",
        "        for h in range(self.horizon):\n",
        "            action = self.policy[h, state]\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            self.update(state, action, reward, next_state)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            self.t += 1\n",
        "        self.episode += 1\n",
        "        return initial_state, Vpi\n",
        "\n",
        "    def train(self):\n",
        "        # compute true value function (for the regret)\n",
        "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
        "        regret = np.zeros((self.nb_repetitions, self.nb_episodes + 1))\n",
        "        self.episode_value = []\n",
        "\n",
        "        for rep in range(self.nb_repetitions):\n",
        "            self.reset()\n",
        "            old_regret = 0\n",
        "            for k in tqdm(range(self.nb_episodes)):\n",
        "                init_state, Vpi = self.run_episode()\n",
        "                episode_regret = trueV[0, init_state] - Vpi[0, init_state]\n",
        "                old_regret += episode_regret\n",
        "                regret[rep, k + 1] = old_regret\n",
        "        return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMJTmLPcOwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCBVI:\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        np.random.seed(seed=config['seed'])\n",
        "        self.env = config['env']\n",
        "        self.horizon = config['horizon']\n",
        "        self.scale_factor = config['scale_factor']\n",
        "        self.nb_repetitions = config['repetitions']\n",
        "        self.nb_episodes = config['episodes']\n",
        "        assert config['b_type'] in ['hoeffding', 'bernstein']\n",
        "        self.b_type = config['b_type']\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        S, A = self.env.Ns, self.env.Na\n",
        "        self.delta = 0.1\n",
        "        self.t = 0\n",
        "        self.episode = 0\n",
        "        self.Phat = np.zeros((S, A, S))\n",
        "        self.Rhat = np.zeros((S, A, S))\n",
        "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
        "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
        "        self.policy = np.zeros((self.horizon, S), dtype=np.int)\n",
        "        self.V = np.zeros((self.horizon+1, S))\n",
        "        self.Q = np.zeros((self.horizon+1, S, A))\n",
        "        self.bonus = np.zeros((self.horizon, S, A))\n",
        "        \n",
        "\n",
        "    def get_optimistic_q(self):\n",
        "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
        "        \"\"\"\n",
        "        H = self.horizon\n",
        "        S, A = self.N_sa.shape\n",
        "        self.V.fill(0)\n",
        "        self.Q.fill(0)\n",
        "\n",
        "        states = np.arange(S)\n",
        "\n",
        "        # Simplifying the computation by looking at lines directly\n",
        "        for h in reversed(range(H)):\n",
        "            # Computing the right bonus\n",
        "            if self.b_type == \"hoeffding\":\n",
        "                self.compute_hoeffding_bonus()\n",
        "            if self.b_type == \"bernstein\":\n",
        "                self.compute_bernstein_bonus(h, self.V[h + 1])\n",
        "\n",
        "            bellman_Q = np.sum(self.Phat * (self.Rhat + np.max(self.Q[h + 1],axis=1)), axis=2)\n",
        "            \n",
        "            self.Q[h] = self.bonus[h] + bellman_Q\n",
        "\n",
        "            self.policy[h] = np.argmax(self.Q[h],axis=1)\n",
        "\n",
        "            self.V[h] = np.minimum(H - h + 1., self.Q[h, states, self.policy[h]])\n",
        "\n",
        "\n",
        "    def compute_hoeffding_bonus(self):\n",
        "        \"\"\"Compute Hoeffding-based exploration bonus\n",
        "        \"\"\"\n",
        "        S = self.env.Ns\n",
        "        A = self.env.Na\n",
        "\n",
        "        K = self.nb_episodes\n",
        "        H = self.horizon\n",
        "\n",
        "        N = np.maximum(1.0, self.N_sa) # to avoid div by 0\n",
        "        \n",
        "        # For T we chose $K \\times H$ like in the above paper. \n",
        "        L = np.log(5.0 * S * A * K * H / self.delta)\n",
        "\n",
        "        for h in range(self.horizon):\n",
        "            self.bonus[h] =  7.0 * (H - h + 1.) * L * np.sqrt(1. / N)\n",
        "\n",
        "            # Applying a scale factor like for Bernstein \n",
        "            # to increase the convergence speed\n",
        "            self.bonus[h] *= self.scale_factor\n",
        "\n",
        "\n",
        "    def compute_bernstein_bonus(self, h, Vhp1):\n",
        "        \"\"\"Compute Bernstein-based exploration bonus\n",
        "\n",
        "        Parameters:\n",
        "            h: state\n",
        "            Vhp1: value function at state h+1 (S-dim vector)\n",
        "        \"\"\"\n",
        "        S, A = self.N_sa.shape\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
        "                n = max(1, self.N_sa[s][a])\n",
        "                var, mean = 0, 0\n",
        "                for i in range(S):\n",
        "                    mean += self.Phat[s,a,i] * Vhp1[i]\n",
        "                for i in range(S):\n",
        "                    var += self.Phat[s,a,i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\n",
        "                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon -h + 2) / (3*n)\n",
        "                T2 = np.sqrt(8 * (self.horizon -h + 2)**2  / n)\n",
        "                self.bonus[h,s,a] = self.scale_factor * (T1 + T2)\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the internal statistics\n",
        "        \"\"\"\n",
        "        # Updating the counts for the state/actions\n",
        "        self.N_sa[state, action] += 1\n",
        "        self.N_sas[state, action, next_state] += 1\n",
        "        \n",
        "        # Updating the estimations of the probabilities and rewards\n",
        "        self.Phat[state, action, next_state] = self.N_sas[state, action, next_state] / self.N_sa[state, action]\n",
        "        # The estimate proba of transitioning from s to s' when doing a is the proportion\n",
        "        # of the times this transition happened, on the times we where in s and did a\n",
        "        \n",
        "        # Same principle for the estimate rewards\n",
        "        sum_rewards = reward + self.Rhat[state, action, next_state] * (self.N_sas[state, action, next_state] - 1)\n",
        "        self.Rhat[state, action, next_state] = sum_rewards / self.N_sas[state, action, next_state]\n",
        "        \n",
        "        \n",
        "    def run_episode(self):\n",
        "        episode_reward = 0\n",
        "        state = self.env.reset()\n",
        "        initial_state = state\n",
        "        self.get_optimistic_q()\n",
        "\n",
        "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
        "        self.episode_value.append(Vpi[0, initial_state])\n",
        "\n",
        "        for h in range(self.horizon):\n",
        "            action = self.policy[h, state]\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            self.update(state, action, reward, next_state)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            self.t += 1\n",
        "        self.episode += 1\n",
        "        return initial_state, Vpi\n",
        "    \n",
        "    \n",
        "    def train(self):\n",
        "        # Will be updates with the run_episode\n",
        "        self.episode_value = []\n",
        "\n",
        "        # compute true value function (for the regret)\n",
        "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
        "        regret = np.zeros((self.nb_repetitions, self.nb_episodes+1))\n",
        "\n",
        "        for rep in range(self.nb_repetitions):\n",
        "            self.reset()\n",
        "            old_regret = 0\n",
        "            for k in range(self.nb_episodes):\n",
        "                init_state, Vpi = self.run_episode()\n",
        "                # The regret at k is the diff between optimal and obtained values\n",
        "                episode_regret = trueV[0, init_state] - Vpi[0, init_state]\n",
        "                old_regret += episode_regret\n",
        "                regret[rep, k+1] = old_regret\n",
        "        return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSaZ3xOckmd",
        "colab_type": "text"
      },
      "source": [
        "Define the settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjk1TbBock-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'env': env,\n",
        "    'scale_factor': 0.1, # we use a scaling factor in order to increase the convergence speed\n",
        "    'seed': 1,\n",
        "    'horizon': H,\n",
        "    'episodes': 10000,\n",
        "    'repetitions': 5,\n",
        "    'b_type': 'hoeffding' # [hoeffding, bernstein]\n",
        "}\n",
        "\n",
        "print(\"Current config is:\")\n",
        "pprint(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSI7NR50cyFx",
        "colab_type": "text"
      },
      "source": [
        "Run the agent and compare the behaviour with Hoeffding and Bernstein bound.\n",
        "\n",
        "A picture is automatically generated (it will show the regret average regret of the two algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S3ObC9ncydR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "regret = {}\n",
        "for bound in ['hoeffding', 'bernstein']:\n",
        "    tmp_config = copy.copy(config) \n",
        "    tmp_config['b_type'] = bound\n",
        "    agent = UCBVI(config=tmp_config)\n",
        "    regret[bound] = agent.train()\n",
        "\n",
        "    mean_regret = np.mean(regret[bound], axis=0)\n",
        "    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\n",
        "    x = np.arange(regret[bound].shape[1])\n",
        "    plt.plot(x, mean_regret, label=bound)\n",
        "    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}