{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7C_a8pBTdRcK","colab_type":"text"},"source":["<h1 ><big><center>Object recognition and computer vision 2019/2020</center></big></h1>\n","\n","<h3><big><center><a href=\"http://www.di.ens.fr/~ponce/\">Jean Ponce</a>, <a href=\"http://www.di.ens.fr/~laptev/\">Ivan Laptev</a>, <a href=\"http://lear.inrialpes.fr/~schmid/\">Cordelia Schmid</a> and <a href=\"http://www.di.ens.fr/~josef/\">Josef Sivic</a></center></big></h3>\n","\n","\n","<h2><big><center> Assignment 2: Neural networks</center></big></h2>\n","\n","<h5><big><center>Adapted from practicals from <a href=\"http://nicolas.le-roux.name/\">Nicolas le Roux</a>, \n","  <br><a href=\"http://www.robots.ox.ac.uk/~vgg/practicals/overview/index.html\">Andrea Vedaldi and Andrew Zisserman</a> and <a href=\"https://cs.nyu.edu/~fergus/teaching/vision/\">Rob Fergus</a>\n","  <br> by <a href=\"https://www.di.ens.fr/~varol/\">Gul Varol</a> and <a href=\"https://www.di.ens.fr/~iroccosp/\">Ignacio Rocco</a></center></big></h5>\n","\n","\n","</br>\n","\n","<p align=\"center\">\n","<img height=300px src=\"http://www.di.ens.fr/willow/teaching/recvis/assignment3/images/fullyconn.png\"/></p>\n","<p align=\"center\">Figure 1</p>\n","<p align=\"center\"></p>"]},{"cell_type":"markdown","metadata":{"id":"wcCxTK_AsjRN","colab_type":"text"},"source":["**STUDENT**:  YOUR NAME HERE\n","\n","**EMAIL**:  YOUR EMAIL HERE"]},{"cell_type":"markdown","metadata":{"id":"5Vkl5mVLx5WF","colab_type":"text"},"source":["# Guidelines\n","\n","The purpose of this assignment is that you get hands-on experience with the topics covered in class, which will help you understand these topics better. Therefore, ** it is imperative that you do this assignment yourself. No code sharing will be tolerated. **\n","\n","Once you have completed the assignment, you will submit the `ipynb` file containing **both** code and results. For this, make sure to **run your notebook completely before submitting**.\n","\n","The `ipynb` must be named using the following format: **A2_LASTNAME_Firstname.ipynb**, and submitted in the **class Moodle page**."]},{"cell_type":"markdown","metadata":{"id":"whwqRMjafHSd","colab_type":"text"},"source":["# Goal\n"]},{"cell_type":"markdown","metadata":{"id":"mcyB-zABfI4d","colab_type":"text"},"source":["The goal of this assignment is to get basic knowledge and hands-on experience with training and using neural networks. In Part 1 of the assignment you will implement and experiment with the training and testing of a simple two layer fully-connected neural network, similar to the one depicted in Figure 1 above. In Part 2 you will learn about convolutional neural networks, their motivation, building blocks, and how they are trained. Finally, in part 3 you will train a CNN for classification using the CIFAR-10 dataset."]},{"cell_type":"markdown","metadata":{"id":"fb8b1djrslSq","colab_type":"text"},"source":["# Part 1 - Training a fully connected neural network"]},{"cell_type":"markdown","metadata":{"id":"woMHcTuKcVcE","colab_type":"text"},"source":["## Getting started"]},{"cell_type":"markdown","metadata":{"id":"p-aZ8b6Ko3yQ","colab_type":"text"},"source":["You will be working with a two layer neural network of the following form \n","\n","\\begin{equation}\n","H=\\text{ReLU}(W_i X+B_i)\\\\\n","Y=W_oH+B_o\n","\\tag{1}\n","\\end{equation}\n","\n","where $X$ is the input, $Y$ is the output, $H$ is the hidden layer, and $W_i$, $W_o$, $B_i$ and $B_o$ are the network parameters that need to be trained. Here the subscripts $i$ and $o$ stand for the *input* and *output* layer, respectively. This network was also discussed in the class and is illustrated in the above figure where the input units are shown in green, the hidden units in blue and the output in yellow. This network is implemented in the function `nnet_forward_logloss`.\n","\n","You will train the parameters of the network from labelled training data $\\{X^n,Y^n\\}$ where $X^n$ are points in $\\mathbb{R}^2$ and $Y^n\\in\\{-1,1\\}$ are labels for each point. You will use the stochastic gradient descent algorithm discussed in the class to minimize the loss of the network on the training data given by \n","\n","\\begin{equation}\n","L=\\sum_n s(Y^n,\\bar{Y}(X^n))\n","\\tag{2}\n","\\end{equation}\n","\n","where $Y^n$ is the target label for the n-th example and $\\bar{Y}(X^n)$ is the networkâ€™s output for the n-th example $X^n$. The skeleton of the training procedure is provided in the `train_loop` function. \n","\n","We will use the logistic loss, which has the following form:\n","\n","\\begin{equation}\n","s(Y, \\bar{Y}(X))=\\log(1+\\exp(-Y. \\bar{Y}(X))\n","\\tag{3}\n","\\end{equation}\n","\n","where $Y$ is the target label and $\\bar{Y}(X)$ is the output of the network for input example $X$. With the logistic loss, the output of the network can be interpreted as a probability $P(\\text{class}=1|X) =\\sigma(X)$ , where $\\sigma(X) =1/(1+\\exp(-X))$ is the sigmoid function. Note also that $P(\\text{class}=-1|X)=1-P(\\text{class}=1|X)$."]},{"cell_type":"code","metadata":{"id":"tn4onV1g9clm","colab_type":"code","colab":{}},"source":["from IPython import display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.io as sio\n","\n","\n","def decision_boundary_nnet(X, Y, Wi, bi, Wo, bo):\n","    x_min, x_max = -2, 4\n","    y_min, y_max = -5, 3\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, .05),\n","                     np.arange(y_min, y_max, .05))\n","\n","    XX = np.vstack((xx.ravel(), yy.ravel())).T\n","    input_hidden = np.dot(XX, Wi) + bi\n","    hidden = np.maximum(input_hidden, 0)\n","    Z = np.dot(hidden, Wo) + bo\n","\n","    # Put the result into a color plot\n","    Z = Z.reshape(xx.shape)\n","    plt.contourf(xx, yy, Z > 0, cmap=plt.cm.Paired)\n","    plt.axis('off')\n","\n","    # Plot also the training points\n","    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='winter')\n","    plt.axis([-2, 4, -5, 3])\n","    plt.draw()\n","\n","\n","def sigm(x):\n","    # Returns the sigmoid of x.\n","    small_x = np.where(x < -20)  # Avoid overflows.\n","    sigm_x = 1/(1 + np.exp(-x))\n","    if type(sigm_x) is np.ndarray:\n","        sigm_x[small_x] = 0.0\n","    return sigm_x\n","\n","\n","def nnet_forward_logloss(X, Y, Wi, bi, Wo, bo):\n","    '''\n","    Compute the output Po, Yo and the loss of the network for the input X\n","    This is a 2 layer (1 hidden layer network)\n","\n","    Input:\n","        X ... (in R^2) set of input points, one per column\n","        Y ... {-1,1} the target values for the set of points X\n","        Wi, bi, Wo, bo ... parameters of the network\n","\n","    Output: \n","       Po ... probabilisitc output of the network P(class=1 | x) \n","                  Po is in <0 1>. \n","                  Note: P(class=-1 | x ) = 1 - Po\n","       Yo ... output of the network Yo is in <-inf +inf>\n","       loss ... logistic loss of the network on examples X with ground target\n","                    values Y in {-1,1}\n","    '''\n","    # Hidden layer\n","    hidden = np.maximum(np.dot(X, Wi) + bi, 0)\n","    # Output of the network\n","    Yo = np.dot(hidden, Wo) + bo\n","    # Probabilistic output\n","    Po = sigm(Yo)\n","    # Logistic loss\n","    loss = np.log(1 + np.exp( -Y * Yo)) \n","    return Po, Yo, loss\n","\n","\n","# Load the training data\n","!wget -q http://www.di.ens.fr/willow/teaching/recvis18/assignment2/double_moon_train1000.mat\n","train_data = sio.loadmat('./double_moon_train1000.mat', squeeze_me=True)\n","Xtr = train_data['X']\n","Ytr = train_data['Y']\n","# Load the validation data\n","!wget -q http://www.di.ens.fr/willow/teaching/recvis18/assignment2/double_moon_val1000.mat\n","val_data = sio.loadmat('./double_moon_val1000.mat', squeeze_me=True)\n","Xval = val_data['X']\n","Yval = val_data['Y']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAwOzAhNqUpZ","colab_type":"text"},"source":["## Computing gradients of the loss with respect to network parameters "]},{"cell_type":"markdown","metadata":{"id":"Ta2LUkw7Jnd9","colab_type":"text"},"source":["### :: TASK 1.1 ::\n","Derive the form of the gradient of the logistic loss (3) with respect to the parameters of the network $W_i$, $W_o$, $B_i$ and $B_o$.  *Hint:* Use the chain rule as discussed in the class."]},{"cell_type":"markdown","metadata":{"id":"Mm0supnJpWOl","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"dOU3PyrWpilT","colab_type":"text"},"source":["### :: TASK 1.2 ::\n","\n","Following your derivation, implement the gradient computation in the function `gradient_nn`. See the code for the description of the required inputs / outputs of this function."]},{"cell_type":"code","metadata":{"id":"WvEHm2Ah9d3k","colab_type":"code","colab":{}},"source":["def gradient_nn(X, Y, Wi, bi, Wo, bo):\n","    '''\n","    Compute gradient of the logistic loss of the neural network on example X with\n","    target label Y, with respect to the parameters Wi,bi,Wo,bo.\n","\n","    Input:\n","        X ... 2d vector of the input example\n","        Y ... the target label in {-1,1}   \n","        Wi,bi,Wo,bo ... parameters of the network\n","        Wi ... [dxh]\n","        bi ... [h]\n","        Wo ... [h]\n","        bo ... 1\n","        where h... is the number of hidden units\n","              d... is the number of input dimensions (d=2)\n","\n","    Output: \n","        grad_s_Wi [dxh] ... gradient of loss s(Y,Y(X)) w.r.t  Wi\n","        grad_s_bi [h]   ... gradient of loss s(Y,Y(X)) w.r.t. bi\n","        grad_s_Wo [h]   ... gradient of loss s(Y,Y(X)) w.r.t. Wo\n","        grad_s_bo 1     ... gradient of loss s(Y,Y(X)) w.r.t. bo\n","    '''\n","\n","    ##########################\n","    #                        #\n","    #  WRITE YOUR CODE HERE  #\n","    #                        #\n","    ##########################\n","    # grad_s_Wi = \n","    # grad_s_bi = \n","    # grad_s_Wo = \n","    # grad_s_bo = \n","    ##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHTrzVASqaO1","colab_type":"text"},"source":["## Numerically verify the gradients\n","Here you will numerically verify that your analytically computed gradients in function `gradient_nn` are correct. "]},{"cell_type":"markdown","metadata":{"id":"AeYWVgB340-f","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"LAD1a_ZuJrS-","colab_type":"text"},"source":["### :: TASK 1.3 ::\n","Write down the general formula for numerically computing the approximate derivative of the loss $s(\\theta)$, with respect to the parameter $\\theta_i$ using finite differencing.  *Hint: use the first order Taylor expansion of loss $s(\\theta+\\Delta \\theta)$ around point $\\theta$. *"]},{"cell_type":"markdown","metadata":{"id":"ssydNgTWq4ht","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"C-tzggJaq8JK","colab_type":"text"},"source":["Following the general formula, `gradient_nn_numerical` function numerically computes the derivatives of the loss function with respect to all the parameters of the network $W_i$, $W_o$, $B_i$ and $B_o$:"]},{"cell_type":"code","metadata":{"id":"Sj2aH1Qs9hAB","colab_type":"code","colab":{}},"source":["def gradient_nn_numerical(X, Y, Wi, bi, Wo, bo):\n","    '''\n","    Compute numerical gradient of the logistic loss of the neural network on\n","    example X with target label Y, with respect to the parameters Wi,bi,Wo,bo.\n","\n","    Input:\n","       X ... 2d vector of the input example\n","       Y ... the target label in {-1,1}   \n","       Wi, bi, Wo, bo ... parameters of the network\n","       Wi ... [dxh]\n","       bi ... [h]\n","       Wo ... [h]\n","       bo ... 1\n","       where h... is the number of hidden units\n","             d... is the number of input dimensions (d=2)\n","\n","    Output: \n","       grad_s_Wi_numerical [dxh] ... gradient of loss s(Y,Y(X)) w.r.t  Wi\n","       grad_s_bi_numerical [h]   ... gradient of loss s(Y,Y(X)) w.r.t. bi\n","       grad_s_Wo_numerical [h]   ... gradient of loss s(Y,Y(X)) w.r.t. Wo\n","       grad_s_bo_numerical 1     ... gradient of loss s(Y,Y(X)) w.r.t. bo\n","    '''\n","\n","    eps = 1e-8\n","    grad_s_Wi_numerical = np.zeros(Wi.shape)\n","    grad_s_bi_numerical = np.zeros(bi.shape)\n","    grad_s_Wo_numerical = np.zeros(Wo.shape)\n","\n","    for i in range(Wi.shape[0]):\n","        for j in range(Wi.shape[1]):\n","            dummy, dummy, pos_loss = nnet_forward_logloss(X, Y, sumelement_matrix(Wi, i, j, +eps), bi, Wo, bo)\n","            dummy, dummy, neg_loss = nnet_forward_logloss(X, Y, sumelement_matrix(Wi, i, j, -eps), bi, Wo, bo)\n","            grad_s_Wi_numerical[i, j] = (pos_loss - neg_loss)/(2*eps)\n","\n","    for i in range(bi.shape[0]):\n","        dummy, dummy, pos_loss = nnet_forward_logloss(X, Y, Wi, sumelement_vector(bi, i, +eps), Wo, bo)\n","        dummy, dummy, neg_loss = nnet_forward_logloss(X, Y, Wi, sumelement_vector(bi, i, -eps), Wo, bo)\n","        grad_s_bi_numerical[i] = (pos_loss - neg_loss)/(2*eps)\n","\n","    for i in range(Wo.shape[0]):\n","        dummy, dummy, pos_loss = nnet_forward_logloss(X, Y, Wi, bi, sumelement_vector(Wo, i, +eps), bo)\n","        dummy, dummy, neg_loss = nnet_forward_logloss(X, Y, Wi, bi, sumelement_vector(Wo, i, -eps), bo)\n","        grad_s_Wo_numerical[i] = (pos_loss - neg_loss)/(2*eps)\n","\n","    dummy, dummy, pos_loss = nnet_forward_logloss(X, Y, Wi, bi, Wo, bo+eps)\n","    dummy, dummy, neg_loss = nnet_forward_logloss(X, Y, Wi, bi, Wo, bo-eps)\n","    grad_s_bo_numerical = (pos_loss - neg_loss)/(2*eps)\n","\n","    return grad_s_Wi_numerical, grad_s_bi_numerical, grad_s_Wo_numerical, grad_s_bo_numerical\n","\n","\n","def sumelement_matrix(X, i, j, element):\n","    Y = np.copy(X)\n","    Y[i, j] = X[i, j] + element\n","    return Y\n","\n","\n","def sumelement_vector(X, i, element):\n","    Y = np.copy(X)\n","    Y[i] = X[i] + element\n","    return Y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMWu1J04a3B6","colab_type":"text"},"source":["### :: TASK 1.4 ::\n","\n","Run the following code snippet and understand what it is doing. `gradcheck` function checks that the analytically computed derivative using function `gradient_nn` (e.g. `grad_s_bo`) at the same training example $\\{X,Y\\}$ is the same (up to small errors) as your numerically computed value of the derivative using function `gradient_nn_numerical` (e.g. `grad_s_bo_numerical`). Make sure the output is `SUCCESS` to move on to the next task."]},{"cell_type":"code","metadata":{"id":"vg1gLuEoaTJb","colab_type":"code","colab":{}},"source":["def gradcheck():\n","    '''\n","    Check that the numerical and analytical gradients are the same up to eps\n","    '''\n","    h = 3 # number of hidden units\n","    eps = 1e-6\n","    for i in range(1000):\n","        # Generate random input/output/weight/bias\n","        X  = np.random.randn(2)\n","        Y  = 2* np.random.randint(2) - 1 # {-1, 1}\n","        Wi = np.random.randn(X.shape[0], h)\n","        bi = np.random.randn(h)\n","        Wo = np.random.randn(h)\n","        bo = np.random.randn(1)\n","        # Compute analytical gradients\n","        grad_s_Wi, grad_s_bi, grad_s_Wo, grad_s_bo = gradient_nn(X, Y, Wi, bi, Wo, bo)\n","        # Compute numerical gradients\n","        grad_s_Wi_numerical, grad_s_bi_numerical, grad_s_Wo_numerical, grad_s_bo_numerical = gradient_nn_numerical(X, Y, Wi, bi, Wo, bo)\n","        # Compute the difference between analytical and numerical gradients\n","        delta_Wi = np.mean(np.abs(grad_s_Wi - grad_s_Wi_numerical))\n","        delta_bi = np.mean(np.abs(grad_s_bi - grad_s_bi_numerical))\n","        delta_Wo = np.mean(np.abs(grad_s_Wo - grad_s_Wo_numerical))\n","        delta_bo = np.abs(grad_s_bo - grad_s_bo_numerical)\n","        # Difference larger than a threshold\n","        if ( delta_Wi > eps or delta_bi > eps or delta_Wo > eps or delta_bo > eps):\n","            return False\n","\n","    return True \n","\n","\n","# Check gradients\n","if gradcheck():\n","    print('SUCCESS: Passed gradcheck.')\n","else:\n","    print('FAILURE: Fix gradient_nn and/or gradient_nn_aprox implementation.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4uKQMkMp7aH","colab_type":"text"},"source":["## Training the network using backpropagation and experimenting with different parameters"]},{"cell_type":"markdown","metadata":{"id":"GARESNUSJjfR","colab_type":"text"},"source":["Use the provided code below that calls the `train_loop` function. Set the number of hidden units to 7 by setting $h=7$ in the code and set the learning rate to 0.02 by setting `lrate = 0.02`. Run the training code. Visualize the trained hyperplane using the provided function `plot_decision_boundary(Xtr,Ytr,Wi,bi,Wo,bo)`. Show also the evolution of the training and validation errors. Include the decision hyper-plane visualization and the training and validation error plots."]},{"cell_type":"code","metadata":{"id":"OO5sWlHe-6Fc","colab_type":"code","colab":{}},"source":["def train_loop(Xtr, Ytr, Xval, Yval, h, lrate, vis='all', nEpochs=100):\n","    '''\n","    Check that the numerical and analytical gradients are the same up to eps\n","\n","    Input:\n","        Xtr ... Nx2 matrix of training samples\n","        Ytr ... N dimensional vector of training labels\n","        Xval ... Nx2 matrix of validation samples \n","        Yval ... N dimensional vector validation labels\n","        h ... number of hidden units\n","        lrate ... learning rate\n","        vis ... visulaization option ('all' | 'last' | 'never')\n","        nEpochs ... number of training epochs\n","\n","    Output:\n","        tr_error ... nEpochs*nSamples dimensional vector of training error\n","        val_error ... nEpochs*nSamples dimensional vector of validation error\n","    '''\n","\n","    nSamples = Xtr.shape[0]\n","    tr_error = np.zeros(nEpochs*nSamples)\n","    val_error = np.zeros(nEpochs*nSamples)\n","\n","    # Randomly initialize parameters of the model\n","    Wi = np.random.randn(Xtr.shape[1], h)\n","    Wo = np.zeros(h)\n","    bi = np.zeros(h)\n","    bo = 0.\n","\n","    if(vis == 'all' or vis == 'last'):\n","        plt.figure()\n","\n","    for i in range(nEpochs*nSamples):\n","        # Draw an example at random\n","        n = np.random.randint(nSamples)\n","        X = Xtr[n]\n","        Y = Ytr[n]\n","\n","        # Compute gradient \n","        grad_s_Wi, grad_s_bi, grad_s_Wo, grad_s_bo = gradient_nn(X, Y, Wi, bi, Wo, bo)\n","\n","        # Gradient update\n","        Wi -= lrate*grad_s_Wi\n","        Wo -= lrate*grad_s_Wo\n","        bi -= lrate*grad_s_bi\n","        bo -= lrate*grad_s_bo\n","\n","        # Compute training error\n","        Po, Yo, loss    = nnet_forward_logloss(Xtr, Ytr, Wi, bi, Wo, bo)\n","        Yo_class        = np.sign(Yo)\n","        tr_error[i]     = 100*np.mean(Yo_class != Ytr)\n","\n","        # Compute validation error\n","        Pov, Yov, lossv = nnet_forward_logloss(Xval, Yval, Wi, bi, Wo, bo)\n","        Yov_class       = np.sign(Yov)\n","        val_error[i]    = 100*np.mean(Yov_class != Yval)\n","\n","        # Plot (at every epoch if visualization is 'all', only at the end if 'last')\n","        if(vis == 'all' and i%nSamples == 0) or (vis == 'last' and i == nEpochs*nSamples - 1):\n","            # Draw the decision boundary.\n","            plt.clf()\n","            plt.title('p = %d, Iteration = %.d, Error = %.3f' % (h, i/nSamples, tr_error[i]))\n","            decision_boundary_nnet(Xtr, Ytr, Wi, bi, Wo, bo)\n","            display.display(plt.gcf(), display_id=True)\n","            display.clear_output(wait=True)\n","\n","    if(vis == 'all'):\n","        # Plot the evolution of the training and test errors\n","        plt.figure()\n","        plt.plot(tr_error, label='training')\n","        plt.plot(val_error, label='validation')\n","        plt.legend()\n","        plt.title('Training/validation errors: %.2f%% / %.2f%%' % (tr_error[-1], val_error[-1]))\n","    return tr_error, val_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V26FVQc2CnhE","colab_type":"code","colab":{}},"source":["# Run training\n","h = 7\n","lrate = .02\n","tr_error, val_error = train_loop(Xtr, Ytr, Xval, Yval, h, lrate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EYL5XZqSykB","colab_type":"text"},"source":["### :: TASK 1.6 ::\n","\n","**Random initializations.** Repeat this procedure 5 times from 5 different random initializations. Record for each run the final training and validation errors. Did the network always converge to zero training error? Summarize your final training and validation errors into a table for the 5 runs. You do not need to include the decision hyper-plane visualizations. Note: to speed-up the training you can plot the visualization figures less often (or never) and hence speed-up the training."]},{"cell_type":"code","metadata":{"id":"Hz8jSVIrea2o","colab_type":"code","colab":{}},"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwi-Ei8C266K","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"e_CjtAWrTQjY","colab_type":"text"},"source":["### :: SAMPLE TASK ::\n","\n","For this task, the answer is given. Run the given code and answer Task 1.8 similarly.\n","\n","**Learning rate:**\n","\n","Keep $h=7$ and change the learning rate to values $\\text{lrate} = \\{2, 0.2, 0.02, 0.002\\}$. For each of these values run the training procedure 5 times and observe the training behaviour. You do not need to include the decision hyper-plane visualizations in your answer.\n","\n","**- Make one figure** where *final* error for (i) training and (ii) validation sets are superimposed. $x$-axis should be the different values of the learning rate, $y$-axis the error *mean* across 5 runs. Show the standard deviation with error bars and make sure to label each plot with a legend.\n","\n","**- Make another figure** where *training error evolution* for each learning rate is superimposed. $x$-axis should be the iteration number, $y$-axis the training error *mean* across 5 runs for a given learning rate. Show the standard deviation with error bars and make sure to label each curve with a legend."]},{"cell_type":"code","metadata":{"id":"p2bfvOBaTaLd","colab_type":"code","colab":{}},"source":["nEpochs = 40\n","trials = 5\n","lrates = [2, 0.2, 0.02, 0.002]\n","plot_data_lr = np.zeros((2, trials, len(lrates), nEpochs*1000))\n","h = 7\n","for j, lrate in enumerate(lrates):\n","    print('LR = %f' % lrate)\n","    for i in range(trials):\n","        tr_error, val_error = train_loop(Xtr, Ytr, Xval, Yval, h, lrate, vis='never', nEpochs=nEpochs)\n","        plot_data_lr[0, i, j, :] = tr_error\n","        plot_data_lr[1, i, j, :] = val_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjM_zM_KLZE8","colab_type":"code","colab":{}},"source":["plt.errorbar(np.arange(len(lrates)), plot_data_lr[0, :, :, -1].mean(axis=0), yerr=plot_data_lr[0, :, :, -1].std(axis=0), label='Training')\n","plt.errorbar(np.arange(len(lrates)), plot_data_lr[1, :, :, -1].mean(axis=0), yerr=plot_data_lr[0, :, :, -1].std(axis=0), label='Validation')\n","plt.xticks(np.arange(len(lrates)), lrates)\n","plt.xlabel('learning rate')\n","plt.ylabel('error')\n","plt.legend()\n","\n","# Plot the evolution of the training loss for each learning rate\n","plt.figure()\n","for j, lrate in enumerate(lrates):\n","    x = np.arange(plot_data_lr.shape[3])\n","    # Mean training loss over trials\n","    y = plot_data_lr[0, :, j, :].mean(axis=0)\n","    # Standard deviation over trials\n","    ebar = plot_data_lr[0, :, j, :].std(axis=0)\n","    # Plot\n","    markers, caps, bars = plt.errorbar(x, y, yerr=ebar, label='LR = ' + str(lrate))\n","    # Make the error bars transparent\n","    [bar.set_alpha(0.01) for bar in bars]\n","plt.legend()\n","plt.xlabel('iterations')\n","plt.ylabel('training error')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOgata_I4WPJ","colab_type":"text"},"source":["### :: TASK 1.7 ::\n","\n","**- Briefly discuss** the different behaviour of the training for different learning rates. How many iterations does it take to converge or does it converge at all? Which learning rate is  better and why?"]},{"cell_type":"markdown","metadata":{"id":"eBsG2-u9SBro","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"WsJO6EBPTWTj","colab_type":"text"},"source":["### :: TASK 1.8 ::\n","**The number of hidden units:**\n","\n","Set the learning rate to 0.02 and change the number of hidden units $h = \\{1, 2, 5, 7, 10, 100\\}$. For each of these values run the training procedure 5 times and observe the training behaviour\n","\n","**-Visualize** one decision hyper-plane per number of hidden units.\n","\n","**-Make one figure** where *final* error for (i) training and (ii) validation sets are superimposed. $x$-axis should be the different values of the number of hidden units, $y$-axis the error *mean* across 5 runs. Show the standard deviation with error bars and make sure to label each plot with a legend.\n","\n","**-Make another figure** where *training error evolution* for each number of hidden units is superimposed. $x$-axis should be the iteration number, $y$-axis the training error *mean* across 5 runs for a given learning rate. Show the standard deviation with error bars and make sure to label each curve with a legend.\n","\n","**-Briefly discuss** the different behaviours for the different numbers of hidden units."]},{"cell_type":"code","metadata":{"id":"2Rpv8jxTTavK","colab_type":"code","colab":{}},"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0uZHF7leR8Q4","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"z0SbVYBrsJxV","colab_type":"text"},"source":["# Part 2 - Building blocks of a CNN\n","\n","This part introduces typical CNN building blocks, such as ReLU units and linear filters. For a motivation for using CNNs over fully-connected neural networks, see [[Le Cun, et al, 1998]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n"]},{"cell_type":"markdown","metadata":{"id":"PHzt-qQ0e_nu","colab_type":"text"},"source":["## Install PyTorch"]},{"cell_type":"code","metadata":{"id":"_hat63BGNDHo","colab_type":"code","colab":{}},"source":["!pip install torch torchvision\n","import torch\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQeZd0s0rICr","colab_type":"text"},"source":["## Convolution\n","\n","A feed-forward neural network can be thought of as the composition of number of functions \n","$$\n","f(\\mathbf{x}) = f_L(\\dots f_2(f_1(\\mathbf{x};\\mathbf{w}_1);\\mathbf{w}_2)\\dots),\\mathbf{w}_{L}).\n","$$\n","Each function $f_l$ takes as input a datum $\\mathbf{x}_l$ and a parameter vector $\\mathbf{w}_l$ and produces as output a datum $\\mathbf{x}_{l+1}$. While the type and sequence of functions is usually handcrafted, the parameters $\\mathbf{w}=(\\mathbf{w}_1,\\dots,\\mathbf{w}_L)$ are *learned from data* in order to solve a target problem, for example classifying images or sounds.\n","\n","In a *convolutional neural network* data and functions have additional structure. The data $\\mathbf{x}_1,\\dots,\\mathbf{x}_n$ are images, sounds, or more in general maps from a lattice$^1$ to one or more real numbers. In particular, since the rest of the practical will focus on computer vision applications, data will be 2D arrays of pixels. Formally, each $\\mathbf{x}_i$ will be a $M \\times N \\times K$ real array of $M \\times N$ pixels and $K$ channels per pixel. Hence the first two dimensions of the array span space, while the last one spans channels. Note that only the input $\\mathbf{x}=\\mathbf{x}_1$ of the network is an actual image, while the remaining data are intermediate *feature maps*.\n","\n","The second property of a CNN is that the functions $f_l$ have a *convolutional structure*. This means that $f_l$ applies to the input map $\\mathbf{x}_l$ an operator that is *local and translation invariant*. Examples of convolutional operators are applying a bank of linear filters to $\\mathbf{x}_l$. \n","\n","In this part we will familiarise ourselves with a number of such convolutional and non-linear operators. The first one is the regular *linear convolution* by a filter bank. We will start by focusing our attention on a single function relation as follows:\n","$$\n"," f: \\mathbb{R}^{M\\times N\\times K} \\rightarrow \\mathbb{R}^{M' \\times N' \\times K'},\n"," \\qquad \\mathbf{x} \\mapsto \\mathbf{y}.\n","$$\n","\n","$^1$A two-dimensional *lattice* is a discrete grid embedded in $R^2$, similar for example to a checkerboard."]},{"cell_type":"code","metadata":{"id":"rT_ApQEEv9xA","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torchvision\n","\n","# Download an example image\n","!wget -q http://www.di.ens.fr/willow/teaching/recvis/assignment3/images/peppers.png\n","# Read the image \n","x = np.asarray(Image.open('peppers.png'))/255.0\n","# Print the size of x. Third dimension (=3) corresponds to the R, G, B channels\n","print(x.shape)\n","# Visualize the input x\n","plt.imshow(x)\n","# Convert to torch tensor\n","x = torch.from_numpy(x).permute(2, 0, 1).float()\n","# Prepare it as a batch\n","x = x.unsqueeze(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mgIxfBDyv-Xw","colab_type":"text"},"source":["This should display an image of bell peppers.\n","\n","Next, we create a convolutional layer with a bank of 10 filters of dimension $5 \\times 5 \\times 3$ whose coefficients are initialized randomly. This uses the [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) module from PyTorch:"]},{"cell_type":"code","metadata":{"id":"d2pBSrU_wDMe","colab_type":"code","colab":{}},"source":["# Create a convolutional layer and a random bank of linear filters\n","conv = torch.nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=0, bias=False)\n","print(conv.weight.size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cq168b5EwGmY","colab_type":"text"},"source":["**Remark:** You might have noticed that the `bias` argument to the `torch.nn.Conv2d` function is the empty matrix `false`. It can be otherwise used to pass a vector of bias terms to add to the output of each filter.\n","\n","Note that `conv.weight` has four dimensions, packing 10 filters. Note also that each filter is not flat, but rather a volume containing three slices. The next step is applying the filter to the image."]},{"cell_type":"code","metadata":{"id":"SuKJUfeswIlx","colab_type":"code","colab":{}},"source":["# Apply the convolution operator\n","y = conv(x)\n","# Observe the input/output sizes\n","print(x.size())\n","print(y.size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0JRIbSrwOci","colab_type":"text"},"source":["The variable `y` contains the output of the convolution. Note that the filters are three-dimensional. This is because they operate on a tensor $\\mathbf{x}$ with $K$ channels. Furthermore, there are $K'$ such filters, generating a $K'$ dimensional map $\\mathbf{y}$.\n","\n","We can now visualise the output `y` of the convolution. In order to do this, use the `torchvision.utils.make_grid` function to display an image for each feature channel in `y`:"]},{"cell_type":"code","metadata":{"id":"MFNQWl1qwQmi","colab_type":"code","colab":{}},"source":["# Visualize the output y\n","def vis_features(y):\n","    # Organize it into 10 grayscale images\n","    out = y.permute(1, 0, 2, 3)\n","    # Scale between [0, 1]\n","    out = (out - out.min().expand(out.size())) / (out.max() - out.min()).expand(out.size())\n","    # Create a grid of images\n","    out = torchvision.utils.make_grid(out, nrow=5)\n","    # Convert to numpy image\n","    out = np.transpose(out.detach().numpy(), (1, 2, 0))\n","    # Show\n","    plt.imshow(out)\n","    # Remove grid\n","    plt.gca().grid(False)\n","\n","vis_features(y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSBp8z-kwTNs","colab_type":"text"},"source":["So far filters preserve the resolution of the input feature map. However, it is often useful to *downsample the output*. This can be obtained by using the `stride` option in `torch.nn.Conv2d`:"]},{"cell_type":"code","metadata":{"id":"HKolxPiawVD0","colab_type":"code","colab":{}},"source":["# Try again, downsampling the output\n","conv_ds = torch.nn.Conv2d(3, 10, kernel_size=5, stride=16, padding=0, bias=False)\n","y_ds = conv_ds(x)\n","print(x.size())\n","print(y_ds.size())\n","vis_features(y_ds)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAaqll6lwYZ0","colab_type":"text"},"source":["Applying a filter to an image or feature map interacts with the boundaries, making the output map smaller by an amount proportional to the size of the filters. If this is undesirable, then the input array can be padded with zeros by using the `pad` option:"]},{"cell_type":"code","metadata":{"id":"el7nluidwZ9o","colab_type":"code","colab":{}},"source":["# Try padding\n","conv_pad = torch.nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2, bias=False)\n","y_pad = conv_pad(x)\n","print(x.size())\n","print(y_pad.size())\n","vis_features(y_pad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUMrIkt_wcAT","colab_type":"text"},"source":["In order to consolidate what has been learned so far, we will now design a filter by hand:\n"]},{"cell_type":"code","metadata":{"id":"URH_bUakweDC","colab_type":"code","colab":{}},"source":["w = torch.FloatTensor([[0,  1, 0 ],\n","                      [1, -4, 1 ],\n","                      [0,  1, 0 ]]) \n","w = w.repeat(3, 1).reshape(1, 3, 3, 3)\n","conv_lap = torch.nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1, bias=False)\n","conv_lap.weight = torch.nn.Parameter(w) \n","y_lap = conv_lap(x)\n","print(x.size())\n","print(y_lap.size())\n","\n","plt.figure()\n","vis_features(y_lap)\n","plt.title('filter output')\n","\n","plt.figure()\n","vis_features(-torch.abs(y_lap))\n","plt.title('- abs(filter output)') ;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGOG4V2tntVj","colab_type":"text"},"source":["\n","### :: TASK 2.1 ::\n","* i. What filter have we implemented?\n","* ii. How are the RGB colour channels processed by this filter?\n","* iii. What image structure are detected?"]},{"cell_type":"markdown","metadata":{"id":"aEq2xSlashSv","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"QsOF1ev8n0Bm","colab_type":"text"},"source":["\n","## Non-linear activation functions\n","\n","The simplest non-linearity is obtained by following a linear filter by a *non-linear activation function*, applied identically to each component (i.e. point-wise) of a feature map. The simplest such function is the *Rectified Linear Unit (ReLU)*\n","$$\n","  y_{ijk} = \\max\\{0, x_{ijk}\\}.\n","$$\n","This function is implemented by [`torch.nn.ReLU()`](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU). Run the code below and understand what the filter $\\mathbf{w}$ is doing.\n"]},{"cell_type":"code","metadata":{"id":"XjUKH4mznpgr","colab_type":"code","colab":{}},"source":["w = torch.FloatTensor([[1], [0], [-1]]).repeat(1, 3, 1, 1)\n","w = torch.cat((w, -w), 0)\n","\n","conv = torch.nn.Conv2d(3, 2, kernel_size=(3, 1), stride=1, padding=0, bias=False)\n","conv.weight = torch.nn.Parameter(w)\n","relu = torch.nn.ReLU()\n","\n","y = conv(x)\n","z = relu(y)\n","\n","plt.figure()\n","vis_features(y)\n","plt.figure()\n","vis_features(z)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDOzl2oynxv8","colab_type":"text"},"source":["## Pooling\n","There are several other important operators in a CNN. One of them is *pooling*.  A pooling operator operates on individual feature channels, coalescing nearby feature values into one by the application of a suitable operator. Common choices include max-pooling (using the max operator) or sum-pooling (using summation). For example, *max-pooling* is defined as:\n","$$\n","   y_{ijk} = \\max \\{ y_{i'j'k} : i \\leq i' < i+p, j \\leq j' < j + p \\}\n","$$\n","Max-pooling is implemented by [`torch.nn.MaxPool2d()`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d).\n","\n","### :: TASK 2.2 ::\n","\n","Run the code below to try max-pooling. Look at the resulting image. Can you interpret the result?"]},{"cell_type":"code","metadata":{"id":"Mj1UFStao7AH","colab_type":"code","colab":{}},"source":["mp = torch.nn.MaxPool2d(15, stride=1)\n","y = mp(x)\n","plt.imshow(y.squeeze().permute(1, 2, 0).numpy())\n","plt.gca().grid(False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LKF9TF2pfn-","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"NeNUwmBZKqox","colab_type":"text"},"source":["# Part 3 - Training a CNN\n","\n","This part is an introduction to using PyTorch for training simple neural net models. CIFAR-10 dataset will be used."]},{"cell_type":"markdown","metadata":{"id":"QtR5zV76f1F1","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"-upNbpi5Kqo0","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bo-MFO8ef5SN","colab_type":"text"},"source":["## Parameters"]},{"cell_type":"markdown","metadata":{"id":"mCBoy4tDTx10","colab_type":"text"},"source":["The default values for the learning rate, batch size and number of epochs are given in the \"options\" cell of this notebook. \n","Unless otherwise specified, use the default values throughout this assignment. "]},{"cell_type":"code","metadata":{"id":"aOYr8zrvKqo8","colab_type":"code","colab":{}},"source":["batch_size = 64   # input batch size for training\n","epochs = 10       # number of epochs to train\n","lr = 0.01         # learning rate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zEbMT3_TO4R","colab_type":"text"},"source":["## Warmup\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mxRVsz8RqtXM","colab_type":"text"},"source":["It is always good practice to visually inspect your data before trying to train a model, since it lets you check for problems and get a feel for the task at hand.\n","\n","CIFAR-10 is a dataset of 60,000 color images (32 by 32 resolution) across 10 classes\n","(airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). \n","The train/test split is 50k/10k."]},{"cell_type":"code","metadata":{"id":"5abkUsPtKqpB","colab_type":"code","colab":{}},"source":["# Data Loading\n","# Warning: this cell might take some time when you run it for the first time, \n","#          because it will download the dataset from the internet\n","dataset = 'cifar10'\n","data_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","trainset = datasets.CIFAR10(root='.', train=True, download=True, transform=data_transform)\n","testset = datasets.CIFAR10(root='.', train=True, download=True, transform=data_transform)\n","\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader  = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nCBfDskxkxE8","colab_type":"text"},"source":["### :: TASK 3.1 ::\n","\n","Use `matplotlib` and ipython notebook's visualization capabilities to display some of these images. Display 5 images from the dataset together with their category label. [See this PyTorch tutorial page](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for hints on how to achieve this."]},{"cell_type":"code","metadata":{"id":"OP7lSX8dkxtR","colab_type":"code","colab":{}},"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9PtOx8IUzvm","colab_type":"text"},"source":["\n","## Training a Convolutional Network on CIFAR-10\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-WGuyKKpqxQd","colab_type":"text"},"source":["Start by running the provided training code below. By default it will train on CIFAR-10 for 10 epochs (passes through the training data) with a single layer network. The loss function [cross_entropy](http://pytorch.org/docs/master/nn.html?highlight=cross_entropy#torch.nn.functional.cross_entropy) computes a Logarithm of the Softmax on the output of the neural network, and then computes the negative log-likelihood w.r.t. the given `target`. Note the decrease in training loss and corresponding decrease in validation errors."]},{"cell_type":"code","metadata":{"id":"r5zXKHbtKqpL","colab_type":"code","colab":{}},"source":["def train(epoch, network):\n","    network.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = network(data)\n","        loss = F.cross_entropy(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","def test(network):\n","    network.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        output = network(data)\n","        test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n","        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VchVVxwKqpG","colab_type":"code","colab":{}},"source":["# Single layer network architecture\n","\n","class Net(nn.Module):\n","    def __init__(self, num_inputs, num_outputs):\n","        super(Net, self).__init__()\n","        self.linear = nn.Linear(num_inputs, num_outputs)\n","        self.num_inputs = num_inputs\n","        \n","    def forward(self, input):\n","        input = input.view(-1, self.num_inputs) # reshape input to batch x num_inputs\n","        output = self.linear(input)\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuYRfWFQoYtp","colab_type":"code","colab":{}},"source":["# Train\n","network = Net(3072, 10)\n","optimizer = optim.SGD(network.parameters(), lr=lr)\n","for epoch in range(1, 11):\n","    train(epoch, network)\n","    test(network)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HU7Gg5B5nZPr","colab_type":"text"},"source":["### :: TASK 3.2 ::\n","\n","Add code to create a convolutional network architecture as below.\n","\n","  - Convolution with 5 by 5 filters, 16 feature maps + Tanh nonlinearity.\n","  - 2 by 2 max pooling.\n","  - Convolution with 5 by 5 filters, 128 feature maps + Tanh nonlinearity.\n","  - 2 by 2 max pooling.\n","  - Flatten to vector.\n","  - Linear layer with 64 hidden units + Tanh nonlinearity.\n","  - Linear layer to 10 output units."]},{"cell_type":"code","metadata":{"id":"EvVgXdUFnfx4","colab_type":"code","colab":{}},"source":["class ConvNet(nn.Module):\n","    ##########################\n","    #                        #\n","    #  WRITE YOUR CODE HERE  #\n","    #                        #\n","    ##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlWfWyEgfEfs","colab_type":"text"},"source":["### :: TASK 3.3 ::\n","\n","Some of the functions in a CNN must be non-linear. Why?\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-BbHt6pegaUe","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"oSDcz789fFbn","colab_type":"text"},"source":["### :: TASK 3.4 ::\n","Train the CNN for 20 epochs on the CIFAR-10 training set.\n","\n"]},{"cell_type":"code","metadata":{"id":"-lpwHqRiKqpQ","colab_type":"code","colab":{}},"source":["# Train\n","network=ConvNet()\n","optimizer = optim.SGD(network.parameters(), lr=lr)\n","for epoch in range(1, 21):\n","    train(epoch, network)\n","    test(network)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTqioMPLn88f","colab_type":"text"},"source":["### :: TASK 3.5 ::\n","Plot the first convolutional layer weights as images after the last epoch. (Hint threads: [#1](https://discuss.pytorch.org/t/understanding-deep-network-visualize-weights/2060/2?u=smth) [#2](https://github.com/pytorch/vision#utils) )\n"]},{"cell_type":"code","metadata":{"id":"LGjcFoLFoCUP","colab_type":"code","colab":{}},"source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHpPKbYAfLBM","colab_type":"text"},"source":["### :: TASK 3.6 :: \n","\n","What is the dimensionality of the weights at each layer? How many parameters are there in total in this CNN architecture?"]},{"cell_type":"markdown","metadata":{"id":"oDb_X3BtrTan","colab_type":"text"},"source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"cell_type":"markdown","metadata":{"id":"WmyqkEBybfJL","colab_type":"text"},"source":["\n","\n","## Useful resources\n","\n","  - [PyTorch tutorial](http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)\n","  - [MNIST example](https://github.com/pytorch/examples/tree/master/mnist)"]},{"cell_type":"markdown","metadata":{"id":"IHc1ZdbOsrMs","colab_type":"text"},"source":["## AUTHORSHIP STATEMENT\n","\n","I declare that the preceding work was the sole result of my own effort and that I have not used any code or results from third-parties.\n","\n","FILL IN YOUR NAME HERE"]}]}